{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Homework 1\n",
    "#1) How would you define Machine Learning?\n",
    "\n",
    "#2) What are the differences between Supervised and Unsupervised Learning? \n",
    "#Specify example 3 algorithms for each of these.\n",
    "\n",
    "#3) What are the test and validation set, and why would you want to use them?\n",
    "\n",
    "#4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "\n",
    "#5) How you can explore countionus and discrete variables?\n",
    "\n",
    "#6) Analyse the plot given below. (What is the plot and variable type, \n",
    "#check the distribution and make comment about how you can preproccess it.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 1\n",
    "#1) How would you define Machine Learning?\n",
    "\n",
    "#2) What are the differences between Supervised and Unsupervised Learning? \n",
    "#Specify example 3 algorithms for each of these.\n",
    "\n",
    "#3) What are the test and validation set, and why would you want to use them?\n",
    "\n",
    "#4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "\n",
    "#5) How you can explore countionus and discrete variables?\n",
    "\n",
    "#6) Analyse the plot given below. (What is the plot and variable type, \n",
    "#check the distribution and make comment about how you can preproccess it.)\n",
    "\n",
    "\n",
    "#1\n",
    "\"\"\"\n",
    " ML is a sub section of the artificial intelligence method and\n",
    "it is the learning, improving, and applying the information respectively by a computer without any human intervention \n",
    "and during these processes the math and statistics are used for calculations of the algorithms related to each steps.\n",
    "\"\"\"\n",
    "##########################################################################################3\n",
    "#2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Differences between supervised learning and unsupervised learning\n",
    "\n",
    "1)Supervised learning algorithms are trained using labeled data \n",
    "but Unsupervised learning algorithms are trained using unlabeled data.\n",
    "\n",
    "2)Supervised learning model takes direct feedback to check if it is predicting correct output or not but\n",
    "Unsupervised learning model does not take any feedback.\n",
    "\n",
    "3)Supervised learning model predicts the output but\n",
    "Unsupervised learning model finds the hidden patterns in data.\n",
    "\n",
    "4)In supervised learning, input data is provided to the model along with the output but \n",
    "In unsupervised learning, only input data is provided to the model.\n",
    "\n",
    "5)The goal of supervised learning is to train the model so that it can predict the output when it is given new data but\n",
    "The goal of unsupervised learning is to find the hidden patterns and useful insights from the unknown dataset.\n",
    "\n",
    "6)Supervised learning needs supervision to train the model but \n",
    "Unsupervised learning does not need any supervision to train the model.\n",
    "\n",
    "7)Supervised learning can be categorized in Classification and Regression problems but\n",
    "Unsupervised Learning can be classified in Clustering and Associations problems.\n",
    "\n",
    "8)Supervised learning can be used for those cases where we know the input as well as corresponding outputs but \n",
    "Unsupervised learning can be used for those cases where we have only input data and no corresponding output data.\n",
    "\n",
    "9)Supervised learning model produces an accurate result but \n",
    "Unsupervised learning model may give less accurate result as compared to supervised learning.\n",
    "\n",
    "10)Supervised learning is not close to true Artificial intelligence as in this, we first train the model for each data, \n",
    "and then only it can predict the correct output but\n",
    "Unsupervised learning is more close to the true Artificial Intelligence \n",
    "as it learns similarly as a child learns daily routine things by his experiences.\n",
    "\n",
    "Supervised learning includes various algorithms such as Linear Regression, Logistic Regression, \n",
    "Support Vector Machine, Multi-class Classification, Decision tree, Bayesian Logic, etc \n",
    "\n",
    "Unsupervised learning includes various algorithms such as Clustering, KNN, and Apriori algorithm.\n",
    "\n",
    "\"\"\"\n",
    "###############################################################################################\n",
    "#3\n",
    "\"\"\"\n",
    "Once you have the training data, you need to split it into three sets:\n",
    "\n",
    "a)Traning set: The data you will use to train your model. \n",
    "This will be fed into an algorithm that generates a model. Said model maps inputs to outputs.\n",
    "\n",
    "b)Validation set: This is smaller than the training set, \n",
    "and is used to evaluate the performance of models with different hyperparameter values.\n",
    "It's also used to detect overfitting during the training stages.\n",
    "\n",
    "c)Test set: This set is used to get an idea of the final performance of a model after hyperparameter tuning. \n",
    "It's also useful to get an idea of how different models (SVMs, Neural Networks, Random forests...) perform against each other.\n",
    ".\n",
    " \n",
    " That the “validation dataset” is predominately used to describe \n",
    " the evaluation of models when tuning hyperparameters and data preparation, \n",
    " and the “test dataset” is predominately used to describe \n",
    " the evaluation of a final tuned model when comparing it to other final models.\n",
    " \n",
    " The training set is used to fit the models; the validation set is used to estimate prediction error for model selection;\n",
    " the test set is used for assessment of the generalization error of the final chosen model. \n",
    " Ideally, the test set should be kept in a “vault,” and be brought out only at the end of the data analysis.\n",
    " \n",
    " \n",
    " We use them to evaluate the skill of our  machine learning models.\n",
    "\"\"\"\n",
    "########################################################################################\n",
    "#4\n",
    "\"\"\"\n",
    "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format.\n",
    "Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends,\n",
    "and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues.\n",
    "In the real world data are generally incomplete: \n",
    "lacking attribute values, lacking certain attributes of interest, or containing only aggregate data. \n",
    "Noisy: containing errors or outliers. Inconsistent: containing discrepancies in codes or names.\n",
    "So that to be able to use it we need to preprocess data before sending through a model.\n",
    "\n",
    "By preprocessing data, we:\n",
    "*Make our database more accurate. We eliminate the incorrect or missing values \n",
    "that are there as a result of the human factor or bugs.\n",
    "*Boost consistency. When there are inconsistencies in data or duplicates, it affects the accuracy of the results.\n",
    "*Make the database more complete. We can fill in the attributes that are missing if needed.\n",
    "*Smooth the data. This way we make it easier to use and interpret\n",
    "\n",
    "Data preprocessing is divided into four stages:\n",
    "\n",
    "1-Data cleaning\n",
    "2-Data integration\n",
    "3-Data reduction\n",
    "4-Data transformation\n",
    "\n",
    "1) DATA CLEANING: Data cleaning can be explained as a process to ‘clean’ data \n",
    "by removing outliers, replacing missing values, smoothing noisy data, and correcting inconsistent data.\n",
    "\n",
    "--> Handling Missing values\n",
    "In order to deal with missing data, we can use multiple approaches :\n",
    "1)Removing the training example: The easiest as well as simplest way could be to ignore \n",
    "the particular training example with missing output label(if it is a case of classification problem). \n",
    "But this is usually discouraged as it leads to loss of data.\n",
    "2)Filling in missing value manually: The another way could be to enter \n",
    "the missing values manually in to the system but this is time consuming, and not recommended for huge data sets.\n",
    "3)Using a standard value to replace the missing value: The missing value can be replaced by a global constant such as \n",
    "‘N/A’ or ‘Unknown’. This is a very basic approach which is usually done to prevent data loss and encounter the missing values.\n",
    "4)Using central tendency(mean, median, mode): Based on data distribution, mean (in case of normal distribution) or \n",
    "median (for non-normal distribution) can be used to fill the missing value.\n",
    "\n",
    "--> Handling Noisy data\n",
    "Noise in data is defined as a random variance in a measured variable. \n",
    "In simple words noisy data is data with large amount of additional meaningless information(data may be corrupted, or distorted).\n",
    "To deal with these anomalous values we use data smoothing techniques which are described below :\n",
    "1)Binning: It is the process of dividing a continuous measure in to discrete intervals called bins, \n",
    "and then we look around these bins for noise in data . There are various approaches to binning. \n",
    "Two of them are smoothing by bin means where each bin is replaced by the mean of bin’s values, \n",
    "and smoothing by bin medians where each bin is replaced by the median of bin’s values.\n",
    "2)Regression: Linear regression and multiple linear regression can be used to smooth the data, \n",
    "where the values are conformed to a function.\n",
    "3)Outlier analysis: Approaches such as clustering can be used to detect outliers and deal with them accordingly.\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "2)DATA INTEGRATION :Since data is being collected from multiple sources, data integration has become a vital part of the process.\n",
    "This may lead to redundant and inconsistent data due to various reasons which could result in \n",
    "poor accuracy and speed of data model. To deal with these issues and maintain the data integrity, \n",
    "approaches such as tuple duplication detection and data conflict detection are sought after. \n",
    "The most common approaches to integrate data are:\n",
    "\n",
    "a)Data consolidation: The data is physically bought together to one data store. This usually involves Data Warehousing.\n",
    "\n",
    "b)Data propagation: Copying data from one location to another using applications is called data propagation. \n",
    "It can be synchronous or asynchronous and is event-driven.\n",
    "\n",
    "c)Data virtualization: An interface is used to provide a real-time and unified view of data from multiple sources. \n",
    "The data can be viewed from a single point of access.\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "3) DATA REDUCTION\n",
    "The purpose of data reduction is to have a condensed representation of the data set which is smaller in volume, \n",
    "while maintaining the integrity of original. This results in efficient yet similar results as it improves the quality of data.\n",
    "A few methods to reduce the volume of data are:\n",
    "\n",
    "a)Missing values ratio: Attributes that have more missing values than a threshold are removed.\n",
    "\n",
    "b)Low variance filter: Normalized attributes that have variance (distribution) less than a threshold are also removed, \n",
    "since little changes in data means less information.\n",
    "\n",
    "c)High correlation filter: Normalized attributes that have correlation coefficient more than a threshold are also removed, \n",
    "since similar trends means similar information is carried. \n",
    "Correlation coefficient is usually calculated using statistical methods such as Pearson’s chi-square value etc.\n",
    "\n",
    "d)Principal component analysis: PCA, is a statistical method which reduces the numbers of attributes \n",
    "by grouping highly correlated attributes together. \n",
    "With each iteration, the initial features are reduced to principal components, with greater variance than \n",
    "the original set on the condition that they are uncorrelated with the preceding components. \n",
    "This method suites only for features with numerical values.\n",
    "--------------------------------------------------------------------------------------------------------\n",
    "4) DATA TRANSFORMATION\n",
    "The final step of data preprocessing is transforming the data into form appropriate for Data Modeling. \n",
    "This can be done by:\n",
    "\n",
    "a)Smoothing\n",
    "\n",
    "b)Attribute/feature construction: New attributes are constructed from the given set of attributes.\n",
    "\n",
    "c)Aggregation: Summary and Aggregation operations are applied on the given set of attributes to come up with new attributes.\n",
    "\n",
    "d) Normalization: The data in each attribute is scaled between a smaller range(e.g. 0 to 1 or -1 to 1).\n",
    "\n",
    "e)Discretization: Raw values of the numeric attributes are replaced by discrete or conceptual intervals, \n",
    "which can in return be further organized into higher level intervals.\n",
    "\n",
    "f)Concept hierarchy generation for nominal data: Values for nominal data are generalized to higher order concepts.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "#######################################################################################################\n",
    "#5\n",
    "\"\"\"\n",
    "Discrete data involves round, concrete numbers that are determined by counting. \n",
    "Some synonyms for the word “discrete” include: disconnected, separate and distinct. \n",
    "These could easily be applied to the idea of discrete data.\n",
    "\n",
    "Continuous data involves complex numbers that are measured across a specific time interval.\n",
    "Continuous data refers to the unfixed number of possible measurements between two realistic points.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#################################################################################################\n",
    "#6\n",
    "\"\"\"\n",
    "The given plot is beloged to a continuous data but it has missing points.To be able to use \n",
    "this  we need to preprocess data before sending through a model. We can fix this data by \n",
    "using DATA CLEANING with central tendency(mean, median, mode) and here we can use \n",
    "median (for non-normal distribution) can be used to fill the missing value.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-52f7b15e4fba>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-52f7b15e4fba>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <matplotlib.axes._subplots.AxesSubplot at 0x2b8ced01320>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<matplotlib.axes._subplots.AxesSubplot at 0x2b8ced01320>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
